Monte Carlo simulations are a cornerstone of computational finance, providing robust estimates for pricing derivatives, measuring portfolio risk, and computing Value-at-Risk (VaR). However, these simulations require millions of independent stochastic trials, making classical CPU implementations slow and unsuitable for real-time trading environments.
This research project implements a GPU-accelerated Monte Carlo engine using CUDA C to parallelize simulation of geometric Brownian motion (GBM), option payoff computation, and statistical reduction. Using NVIDIA GPUs, we demonstrate 4.3-4.6× speedup compared to single-core CPU execution for large-scale option pricing problems, achieving up to 47.8× speedup for smaller simulation sizes due to better GPU occupancy.
The project includes serial and parallel implementations, optimization strategies using cuRAND, warp-level parallel reduction, and accuracy validation against the analytical Black-Scholes model. Results demonstrate not only significant performance gains but also high numerical accuracy, with an average error of 0.12%.
The project documents a GPU-accelerated Monte Carlo simulator developed for pricing European options. The work demonstrates the mapping of stochastic financial models to data-parallel GPU kernels, presents a reduction strategy to compute expectation and variance efficiently on the device, and validates empirical estimates against the closed-form Black–Scholes solution. We evaluate accuracy, runtime performance, and throughput, and discuss statistical and numerical considerations relevant to production-grade financial Monte Carlo.
